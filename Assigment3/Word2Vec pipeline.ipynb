{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17a14b1-b93c-46c3-bac5-18ceeb68680d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|frontpage|count|\n+---------+-----+\n|        1| 2129|\n|        0| 9252|\n+---------+-----+\n\n+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+------------+---------+-------------+\n|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|max_comments|max_votes|max_frontpage|\n+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+------------+---------+-------------+\n|39949266|       0|       economist.com|        0|2024-04-06 01:40:43|The AI doctor wil...|The AI doctor wil...|The AI doctor wil...|https://www.econo...|        jdkee|    1|           0|        1|            0|\n|39949302|       2|       bloomberg.com|        0|2024-04-06 01:46:21|Bloomberg - Are y...|           Bloomberg|Meta employees lo...|https://www.bloom...|     Geekette|    2|           2|        2|            0|\n|39949347|       0|        mashable.com|        1|2024-04-06 01:53:25|X's AI chatbot Gr...|Elon Musk's X pus...|X pushed a fake h...|https://mashable....| nickthegreek|    7|           0|        7|            1|\n|39949511|       0|        theverge.com|        0|2024-04-06 02:30:44|Instagram makes m...|Instagram makes m...|Instagram makes m...|https://www.theve...|      mertbio|    1|           0|        1|            0|\n|39949513|       0|     theregister.com|        0|2024-04-06 02:30:59|Techie saved the ...|Techie saved the ...|Techie saved the ...|https://www.there...|CHB0403085482|    1|           0|        1|            0|\n|39949555|       0|     arstechnica.com|        1|2024-04-06 02:40:50|NASA knows what k...|NASA knows what k...|NASA knows what k...|https://arstechni...|      ljoshua|    4|           0|        4|            1|\n|39949723|       0|       brandmelon.ai|        0|2024-04-06 03:24:08|BrandMelon Your P...|BrandMelon Your P...|          BrandMelon|https://www.brand...| marksabanal1|    1|           0|        1|            0|\n|39949859|       2|            nejm.org|        0|2024-04-06 04:08:21|Just a moment...\\...|    Just a moment...|Nirmatrelvir for ...|https://www.nejm....|     Jimmc414|    1|           2|        1|            0|\n|39949882|       0|     theguardian.com|        1|2024-04-06 04:12:24|When security mat...|When security mat...|Working with Qube...|https://www.thegu...|  colinprince|    3|           0|        3|            1|\n|39949891|       0|     theregister.com|        0|2024-04-06 04:14:11|Google open sourc...|Google open sourc...|Google open sourc...|https://www.there...|       nurple|    1|           0|        1|            0|\n|39950009|       0|          fcloud.app|        0|2024-04-06 04:38:02|FCloud | The new ...|FCloud | The new ...|New disruptive cl...|  https://fcloud.app|   jleuthardt|    1|           0|        1|            0|\n|39950058|       0|  washingtonpost.com|        0|2024-04-06 04:47:51|A massive Texas w...|A massive Texas w...|A massive Texas w...|https://www.washi...|  MilnerRoute|    1|           0|        1|            0|\n|39950094|       0|            tembo.io|        0|2024-04-06 04:56:39|What’s Happening ...|What’s Happening ...|What's Happening ...|https://tembo.io/...|  samaysharma|    1|           0|        1|            0|\n|39950186|       0|           nautil.us|        0|2024-04-06 05:21:24|The Marvelous Sea...|The Marvelous Sea...|The Marvelous Sea...|https://nautil.us...|      dnetesn|    1|           0|        1|            0|\n|39950205|       0|  worksinprogress.co|        0|2024-04-06 05:25:23|The entrepreneuri...|The entrepreneuri...|The Entrepreneuri...|https://worksinpr...|      sarimkx|    1|           0|        1|            0|\n|39950455|       0|          acoup.blog|        0|2024-04-06 06:25:37|Collections: Phal...|Collections: Phal...|       Antiochus III|https://acoup.blo...|        Tomte|    1|           0|        1|            0|\n|39950521|       0|          castel.dev|        0|2024-04-06 06:47:32|How I draw figure...|How I draw figure...|I draw figures fo...|https://castel.de...|        Tomte|    1|           0|        1|            0|\n|39950540|       0| nathanielkaiser.xyz|        0|2024-04-06 06:53:40|### Did I do this...|                NULL|Show HN: I built ...|https://nathaniel...|       jombib|    1|           0|        1|            0|\n|39950606|       0|chromewebstore.go...|        0|2024-04-06 07:11:04|Skip to main cont...|Upwork Contract E...|Show HN: Export y...|https://chromeweb...|        tamnv|    1|           0|        1|            0|\n|39950637|       0|           gnome.org|        0|2024-04-06 07:18:08|Custom Artifacts ...|Custom Artifacts ...|Gnome Builder Aba...|https://blogs.gno...| todsacerdoti|    1|           0|        1|            0|\n+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+------------+---------+-------------+\nonly showing top 20 rows\n\n+---------+-----+\n|frontpage|count|\n+---------+-----+\n|        1| 1458|\n|        0| 6211|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "df = spark.read.table(\"default.full_data\")\n",
    "\n",
    "df = df.withColumn(\"frontpage\", col(\"frontpage\").cast(\"integer\"))\n",
    "df.groupBy('frontpage').count().show()\n",
    "# Group by 'aid' and find the maximum values for 'comment', 'vote', and 'frontpage'\n",
    "max_values_df = df.groupBy(\"aid\").agg(\n",
    "    max(\"comments\").alias(\"max_comments\"),\n",
    "    max(\"votes\").alias(\"max_votes\"),\n",
    "    max(\"frontpage\").alias(\"max_frontpage\")\n",
    ")\n",
    "\n",
    "# Join the original DataFrame with max_values_df\n",
    "joined_df = df.join(max_values_df, \"aid\")\n",
    "\n",
    "# Filter out rows where 'comment', 'vote', or 'frontpage' are lower than the maximum values\n",
    "filtered_df = joined_df.filter(\n",
    "    (col(\"comments\") == col(\"max_comments\")) &\n",
    "    (col(\"votes\") == col(\"max_votes\")) &\n",
    "    (col(\"frontpage\") == col(\"max_frontpage\"))\n",
    ")\n",
    "\n",
    "df = filtered_df.dropDuplicates([\"aid\"])\n",
    "\n",
    "df.show()\n",
    "# Show the resulting DataFrame\n",
    "df.groupBy('frontpage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19648a3d-24f0-4bd3-894c-9749e167f5ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.8941716278027485\nRandom Forest AUC: 0.9517453976078563\nGradient Boosted Trees AUC: 0.9563703659366094\nTest Logistic Regression AUC: 0.9439384913156973\nTest Random Forest AUC: 0.9431844195460382\nTest Gradient Boosted Trees AUC: 0.9479390377520399\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, hour, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer, StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "# Convert posted_at to timestamp\n",
    "df = df.withColumn(\"posted_at\", col(\"posted_at\").cast(\"timestamp\"))\n",
    "df = df.withColumn(\"frontpage\", col(\"frontpage\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# Extract hour from posted_at\n",
    "hour_udf = udf(lambda x: x.hour if x else None, IntegerType())\n",
    "df = df.withColumn(\"posted_hour\", hour_udf(col(\"posted_at\")))\n",
    "df = df.fillna({\"title\": \"\", \"source_text\": \"\", \"posted_at\": \"\"})\n",
    "\n",
    "\n",
    "# Impute missing values for numerical columns\n",
    "imputer_numeric = Imputer(inputCols=[\"comments\",\"votes\"], outputCols=[\"comments_imputed\",\"votes_imputed\"], strategy=\"mean\")\n",
    "\n",
    "# Standardize numerical columns\n",
    "assembler_numeric = VectorAssembler(inputCols=[\"comments_imputed\", \"votes_imputed\"], outputCol=\"numeric_features\")\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"numeric_features_scaled\")\n",
    "\n",
    "#Text-mining\n",
    "tokenizer_source_text = Tokenizer(inputCol=\"source_text\", outputCol=\"source_text_words\")\n",
    "remover_source_text = StopWordsRemover(inputCol=\"source_text_words\", outputCol=\"source_text_filtered\")\n",
    "word2vec_source_text = Word2Vec(inputCol=\"source_text_filtered\", outputCol=\"source_text_w2v\", vectorSize=128)\n",
    "\n",
    "tokenizer_title = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "remover_title = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_filtered\")\n",
    "word2vec_title = Word2Vec(inputCol=\"title_filtered\", outputCol=\"title_w2v\", vectorSize=128)\n",
    "\n",
    "# Assemble all features\n",
    "\n",
    "# Assemble all features\n",
    "assembler_all = VectorAssembler(inputCols=[\"posted_hour\", \"numeric_features_scaled\", \"source_text_w2v\", \"title_w2v\"],\n",
    "                                outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[imputer_numeric, assembler_numeric, scaler, \n",
    "                             tokenizer_source_text, remover_source_text, word2vec_source_text,\n",
    "                             tokenizer_title, remover_title, word2vec_title,\n",
    "                             assembler_all])\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_pre_mod, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Split the DataFrame into majority class (frontpage=0) and minority class (frontpage=1)\n",
    "majority_df = train_pre_mod.filter(col(\"frontpage\") == 0)\n",
    "minority_df = train_pre_mod.filter(col(\"frontpage\") == 1)\n",
    "\n",
    "# Sample the majority class DataFrame to match the size of the minority class DataFrame\n",
    "undersampled_majority_df = majority_df.sample(withReplacement=False, fraction=1/4, seed=42)\n",
    "\n",
    "# Concatenate the sampled majority class DataFrame with the minority class DataFrame\n",
    "train = undersampled_majority_df.union(minority_df)\n",
    "\n",
    "# Define models\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "\n",
    "# Create param grids for hyperparameter tuning\n",
    "\n",
    "paramGridLR = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.1, 0.01])\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5])  # Adding elasticNetParam\n",
    "               .addGrid(lr.maxIter, [10, 20])\n",
    "               .build())\n",
    "\n",
    "paramGridRF = (ParamGridBuilder()\n",
    "               .addGrid(rf.numTrees, [10, 20])  # Adding numTrees\n",
    "               .addGrid(rf.maxDepth, [5, 10])        # Adding maxDepth\n",
    "               .build())\n",
    "\n",
    "paramGridGBT = (ParamGridBuilder()\n",
    "                .addGrid(gbt.maxIter, [10, 20])   # Adding maxIter\n",
    "                .addGrid(gbt.maxDepth, [5, 10])       # Adding maxDepth\n",
    "                .build())\n",
    "\n",
    "# Define evaluators\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='frontpage', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Define cross-validators\n",
    "crossvalLR = CrossValidator(estimator=lr,\n",
    "                            estimatorParamMaps=paramGridLR,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=5)\n",
    "\n",
    "crossvalRF = CrossValidator(estimator=rf,\n",
    "                            estimatorParamMaps=paramGridRF,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=5)\n",
    "\n",
    "crossvalGBT = CrossValidator(estimator=gbt,\n",
    "                             estimatorParamMaps=paramGridGBT,\n",
    "                             evaluator=evaluator,\n",
    "                             numFolds=5)\n",
    "\n",
    "# Create pipeline models with cross-validation\n",
    "pipelineLR = Pipeline(stages=[pipeline, crossvalLR])\n",
    "pipelineRF = Pipeline(stages=[pipeline, crossvalRF])\n",
    "pipelineGBT = Pipeline(stages=[pipeline, crossvalGBT])\n",
    "\n",
    "# Train models\n",
    "modelLR = pipelineLR.fit(train)\n",
    "modelRF = pipelineRF.fit(train)\n",
    "modelGBT = pipelineGBT.fit(train)\n",
    "\n",
    "# Evaluate models\n",
    "predictionsLR = modelLR.transform(train)\n",
    "predictionsRF = modelRF.transform(train)\n",
    "predictionsGBT = modelGBT.transform(train)\n",
    "\n",
    "aucLR = evaluator.evaluate(predictionsLR)\n",
    "aucRF = evaluator.evaluate(predictionsRF)\n",
    "aucGBT = evaluator.evaluate(predictionsGBT)\n",
    "\n",
    "print(f\"Logistic Regression AUC: {aucLR}\")\n",
    "print(f\"Random Forest AUC: {aucRF}\")\n",
    "print(f\"Gradient Boosted Trees AUC: {aucGBT}\")\n",
    "\n",
    "test_predictionsLR = modelLR.transform(test)\n",
    "test_predictionsRF = modelRF.transform(test)\n",
    "test_predictionsGBT = modelGBT.transform(test)\n",
    "\n",
    "test_aucLR = evaluator.evaluate(test_predictionsLR)\n",
    "test_aucRF = evaluator.evaluate(test_predictionsRF)\n",
    "test_aucGBT = evaluator.evaluate(test_predictionsGBT)\n",
    "\n",
    "print(f\"Test Logistic Regression AUC: {test_aucLR}\")\n",
    "print(f\"Test Random Forest AUC: {test_aucRF}\")\n",
    "print(f\"Test Gradient Boosted Trees AUC: {test_aucGBT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86cdf9e-e608-4650-8405-11aafd9f2fda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model       AUC  Accuracy  F1 Score    Recall\n0     Logistic Regression  0.981078  0.944796  0.943938  0.975197\n1           Random Forest  0.986876  0.940724  0.943184  0.930665\n2  Gradient-Boosted Trees  0.984733  0.945701  0.947939  0.934047\nConfusion Matrix for Logistic Regression:\n[[1730   44]\n [  78  358]]\nConfusion Matrix for Random Forest:\n[[1651  123]\n [   8  428]]\nConfusion Matrix for Gradient-Boosted Trees:\n[[1657  117]\n [   3  433]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol='frontpage', predictionCol='prediction')\n",
    "\n",
    "# Calculate metrics\n",
    "def evaluate_model(predictions, model_name):\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: 'accuracy'})\n",
    "    f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: 'f1'})\n",
    "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: 'recallByLabel'})\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "metricsLR = evaluate_model(test_predictionsLR, 'Logistic Regression')\n",
    "metricsRF = evaluate_model(test_predictionsRF, 'Random Forest')\n",
    "metricsGBT = evaluate_model(test_predictionsGBT, 'Gradient-Boosted Trees')\n",
    "\n",
    "# Display comparison\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame([metricsLR, metricsRF, metricsGBT])\n",
    "print(results)\n",
    "\n",
    "# Show confusion matrix for each model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_confusion_matrix(predictions):\n",
    "    y_true = predictions.select(\"frontpage\").rdd.flatMap(lambda x: x).collect()\n",
    "    y_pred = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cmLR = compute_confusion_matrix(test_predictionsLR)\n",
    "cmRF = compute_confusion_matrix(test_predictionsRF)\n",
    "cmGBT = compute_confusion_matrix(test_predictionsGBT)\n",
    "\n",
    "print(f\"Confusion Matrix for Logistic Regression:\\n{cmLR}\")\n",
    "print(f\"Confusion Matrix for Random Forest:\\n{cmRF}\")\n",
    "print(f\"Confusion Matrix for Gradient-Boosted Trees:\\n{cmGBT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489df43c-f271-425f-9b02-b15413b631f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to DBFS\n",
    "model_path_LR = \"/dbfs/models/logistic_regression_modelw2v\"\n",
    "model_path_RF = \"/dbfs/models/random_forest_modelw2v\"\n",
    "model_path_GBT = \"/dbfs/models/gradient_boosted_modelw2v\"\n",
    "modelLR.write().overwrite().save(model_path_LR)\n",
    "modelRF.write().overwrite().save(model_path_RF)\n",
    "modelGBT.write().overwrite().save(model_path_GBT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e56236-8437-42cb-8e10-77a58d23a7ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model from DBFS to workspace directory\n",
    "dbutils.fs.cp(model_path_LR, \"dbfs:/FileStore/models/logistic_regression_modelw2v\", recurse=True)\n",
    "dbutils.fs.cp(model_path_RF, \"dbfs:/FileStore/models/random_forest_modelw2v\", recurse=True)\n",
    "dbutils.fs.cp(model_path_GBT, \"dbfs:/FileStore/models/gradient_boosted_modelw2v\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e62a570-c87c-47dc-942b-b4a2c32c8570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-777522522509054>, line 7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m pipeline_model\u001B[38;5;241m.\u001B[39mstages[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Generate SHAP values using the loaded pipeline model\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model)\n",
       "\u001B[1;32m      8\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m explainer(test)\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Continue with the remaining code\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/shap/explainers/_explainer.py:174\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    172\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n",
       "\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 174\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n",
       "\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n",
       "\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: CrossValidatorModel_0e3eeac6a36e"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: CrossValidatorModel_0e3eeac6a36e"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: The passed model is not callable and cannot be analyzed directly with the given masker! Model: CrossValidatorModel_0e3eeac6a36e"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-777522522509054>, line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m pipeline_model\u001B[38;5;241m.\u001B[39mstages[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Generate SHAP values using the loaded pipeline model\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model)\n\u001B[1;32m      8\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m explainer(test)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Continue with the remaining code\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/shap/explainers/_explainer.py:174\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[1;32m    170\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    172\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 174\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
        "\u001B[0;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: CrossValidatorModel_0e3eeac6a36e"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "import shap\n",
    "# Load the pipeline model from DBFS\n",
    "pipeline_model = PipelineModel.load(\"dbfs:/FileStore/models/gradient_boosted_modelw2v\")\n",
    "model = pipeline_model.stages[-1]\n",
    "# Generate SHAP values using the loaded pipeline model\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(test)\n",
    "\n",
    "# Continue with the remaining code\n",
    "shap.summary_plot(shap_values)\n",
    "# shap.plots.waterfall(shap_values[1])\n",
    "# shap.initjs()\n",
    "# shap.plots.force(shap_values[0])\n",
    "# shap.plots.force(shap_values[1])\n",
    "# shap.plots.force(shap_values[:500])\n",
    "# shap.plots.beeswarm(shap_values)\n",
    "# shap.plots.bar(shap_values)\n",
    "# shap.plots.scatter(shap_values[:, \"Handset_9\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9482dbf9-70f7-41b6-ab4a-4412040b36b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3687e4-4163-4814-924d-b305da3b807d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n",
       "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
       "\n",
       "File \u001B[0;32m<command-777522522509101>, line 8\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
       "\u001B[0;32m----> 8\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\u001B[1;32m      9\u001B[0m pipeline_model \u001B[38;5;241m=\u001B[39m PipelineModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/models/gradient_boosted_modelw2v\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:544\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    543\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(\n",
       "\u001B[0;32m--> 544\u001B[0m         \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSparkSession$\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMODULE$\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    545\u001B[0m     )\u001B[38;5;241m.\u001B[39mapplyModifiableSettings(session\u001B[38;5;241m.\u001B[39m_jsparkSession, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
       "\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m session\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1758\u001B[0m, in \u001B[0;36mJVMView.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   1756\u001B[0m message \u001B[38;5;241m=\u001B[39m compute_exception_message(\n",
       "\u001B[1;32m   1757\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m does not exist in the JVM\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name), error_message)\n",
       "\u001B[0;32m-> 1758\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(message)\n",
       "\n",
       "\u001B[0;31mPy4JError\u001B[0m: SparkSession$ does not exist in the JVM\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2116\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n",
       "\u001B[1;32m   2113\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n",
       "\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2116\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_showtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstb\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_pdb:\n",
       "\u001B[1;32m   2118\u001B[0m     \u001B[38;5;66;03m# drop into debugger\u001B[39;00m\n",
       "\u001B[1;32m   2119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebugger(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/DatabricksShell.py:73\u001B[0m, in \u001B[0;36mDatabricksShell._showtraceback\u001B[0;34m(self, etype, evalue, stb)\u001B[0m\n",
       "\u001B[1;32m     71\u001B[0m full_evalue \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(evalue)\n",
       "\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# [ES-1024635]: error message can get so long it crashes the driver.\u001B[39;00m\n",
       "\u001B[0;32m---> 73\u001B[0m truncated_evalue \u001B[38;5;241m=\u001B[39m truncate(full_evalue, limit\u001B[38;5;241m=\u001B[39m\u001B[43mget_max_error_message_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# The traceback contains the error message too, but it contains ansi escape characters for\u001B[39;00m\n",
       "\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# coloring, so we find/replace the full error message with the truncated one.\u001B[39;00m\n",
       "\u001B[1;32m     76\u001B[0m stb \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39mreplace(full_evalue, truncated_evalue) \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m stb]\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/utils.py:104\u001B[0m, in \u001B[0;36mget_max_error_message_length\u001B[0;34m(shell)\u001B[0m\n",
       "\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_max_error_message_length\u001B[39m(shell) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n",
       "\u001B[1;32m    102\u001B[0m     default \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100_000\u001B[39m\n",
       "\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\n",
       "\u001B[0;32m--> 104\u001B[0m         \u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.databricks.driver.ipykernel.maxErrorMessageLength\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[43m                               \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdefault\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(shell, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark_config\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m default)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/conf.py:244\u001B[0m, in \u001B[0;36mSparkConf.get\u001B[0;34m(self, key, defaultValue)\u001B[0m\n",
       "\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 244\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefaultValue\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    246\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:342\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
       "\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mOUTPUT_CONVERTER\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m]\u001B[49m(answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'c'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "KeyError",
        "evalue": "'c'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'c'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
        "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
        "File \u001B[0;32m<command-777522522509101>, line 8\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[0;32m----> 8\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m      9\u001B[0m pipeline_model \u001B[38;5;241m=\u001B[39m PipelineModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/models/gradient_boosted_modelw2v\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:544\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    543\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[0;32m--> 544\u001B[0m         \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSparkSession$\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMODULE$\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m     )\u001B[38;5;241m.\u001B[39mapplyModifiableSettings(session\u001B[38;5;241m.\u001B[39m_jsparkSession, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m session\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1758\u001B[0m, in \u001B[0;36mJVMView.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1756\u001B[0m message \u001B[38;5;241m=\u001B[39m compute_exception_message(\n\u001B[1;32m   1757\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m does not exist in the JVM\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name), error_message)\n\u001B[0;32m-> 1758\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(message)\n",
        "\u001B[0;31mPy4JError\u001B[0m: SparkSession$ does not exist in the JVM",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
        "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2116\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2113\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2116\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_showtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_pdb:\n\u001B[1;32m   2118\u001B[0m     \u001B[38;5;66;03m# drop into debugger\u001B[39;00m\n\u001B[1;32m   2119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebugger(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/DatabricksShell.py:73\u001B[0m, in \u001B[0;36mDatabricksShell._showtraceback\u001B[0;34m(self, etype, evalue, stb)\u001B[0m\n\u001B[1;32m     71\u001B[0m full_evalue \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(evalue)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# [ES-1024635]: error message can get so long it crashes the driver.\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m truncated_evalue \u001B[38;5;241m=\u001B[39m truncate(full_evalue, limit\u001B[38;5;241m=\u001B[39m\u001B[43mget_max_error_message_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# The traceback contains the error message too, but it contains ansi escape characters for\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# coloring, so we find/replace the full error message with the truncated one.\u001B[39;00m\n\u001B[1;32m     76\u001B[0m stb \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39mreplace(full_evalue, truncated_evalue) \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m stb]\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/utils.py:104\u001B[0m, in \u001B[0;36mget_max_error_message_length\u001B[0;34m(shell)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_max_error_message_length\u001B[39m(shell) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m    102\u001B[0m     default \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100_000\u001B[39m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\n\u001B[0;32m--> 104\u001B[0m         \u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.databricks.driver.ipykernel.maxErrorMessageLength\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m                               \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdefault\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(shell, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark_config\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m default)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/conf.py:244\u001B[0m, in \u001B[0;36mSparkConf.get\u001B[0;34m(self, key, defaultValue)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 244\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefaultValue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:342\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mOUTPUT_CONVERTER\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m]\u001B[49m(answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
        "\u001B[0;31mKeyError\u001B[0m: 'c'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "pipeline_model = PipelineModel.load(\"dbfs:/FileStore/models/gradient_boosted_modelw2v\")\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    df = spark.read.json(rdd)\n",
    "    print(\"========= %s =========\" % str(time))    \n",
    "    # Convert posted_at to timestamp\n",
    "    df = df.withColumn(\"posted_at\", col(\"posted_at\").cast(\"timestamp\"))\n",
    "    df = df.withColumn(\"frontpage\", col(\"frontpage\").cast(\"integer\"))\n",
    "\n",
    "    # Extract hour from posted_at\n",
    "    hour_udf = udf(lambda x: x.hour if x else None, IntegerType())\n",
    "    df = df.withColumn(\"posted_hour\", hour_udf(col(\"posted_at\")))\n",
    "    df = df.fillna({\"title\": \"\", \"source_text\": \"\", \"posted_at\": \"\"})\n",
    "\n",
    "    #some preprocessing done when training model\n",
    "    #applying the pipeline to it\n",
    "    df_with_preds = pipeline_model.transform(df)\n",
    "    df_with_preds.select('features','frontpage','predictions').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9037a02c-ed9f-46d5-9d13-8ead7423b979",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa7a721-af62-48b0-9d94-d79d3d50cdbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b054c80a-f6ed-4fc3-a807-c731de7c15dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Victor's notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

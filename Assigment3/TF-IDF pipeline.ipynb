{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ba2c77-0f1c-4a56-a910-c8ab046e30d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"default.full_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3135ca-c499-458b-8340-c250da91ff21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|frontpage|count|\n+---------+-----+\n|        1| 2129|\n|        0| 9252|\n+---------+-----+\n\n+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+------------+---------+-------------+\n|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|max_comments|max_votes|max_frontpage|\n+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+------------+---------+-------------+\n|39949266|       0|       economist.com|        0|2024-04-06 01:40:43|The AI doctor wil...|The AI doctor wil...|The AI doctor wil...|https://www.econo...|        jdkee|    1|           0|        1|            0|\n|39949302|       2|       bloomberg.com|        0|2024-04-06 01:46:21|Bloomberg - Are y...|           Bloomberg|Meta employees lo...|https://www.bloom...|     Geekette|    2|           2|        2|            0|\n|39949347|       0|        mashable.com|        1|2024-04-06 01:53:25|X's AI chatbot Gr...|Elon Musk's X pus...|X pushed a fake h...|https://mashable....| nickthegreek|    7|           0|        7|            1|\n|39949511|       0|        theverge.com|        0|2024-04-06 02:30:44|Instagram makes m...|Instagram makes m...|Instagram makes m...|https://www.theve...|      mertbio|    1|           0|        1|            0|\n|39949513|       0|     theregister.com|        0|2024-04-06 02:30:59|Techie saved the ...|Techie saved the ...|Techie saved the ...|https://www.there...|CHB0403085482|    1|           0|        1|            0|\n|39949555|       0|     arstechnica.com|        1|2024-04-06 02:40:50|NASA knows what k...|NASA knows what k...|NASA knows what k...|https://arstechni...|      ljoshua|    4|           0|        4|            1|\n|39949723|       0|       brandmelon.ai|        0|2024-04-06 03:24:08|BrandMelon Your P...|BrandMelon Your P...|          BrandMelon|https://www.brand...| marksabanal1|    1|           0|        1|            0|\n|39949859|       2|            nejm.org|        0|2024-04-06 04:08:21|Just a moment...\\...|    Just a moment...|Nirmatrelvir for ...|https://www.nejm....|     Jimmc414|    1|           2|        1|            0|\n|39949882|       0|     theguardian.com|        1|2024-04-06 04:12:24|When security mat...|When security mat...|Working with Qube...|https://www.thegu...|  colinprince|    3|           0|        3|            1|\n|39949891|       0|     theregister.com|        0|2024-04-06 04:14:11|Google open sourc...|Google open sourc...|Google open sourc...|https://www.there...|       nurple|    1|           0|        1|            0|\n|39950009|       0|          fcloud.app|        0|2024-04-06 04:38:02|FCloud | The new ...|FCloud | The new ...|New disruptive cl...|  https://fcloud.app|   jleuthardt|    1|           0|        1|            0|\n|39950058|       0|  washingtonpost.com|        0|2024-04-06 04:47:51|A massive Texas w...|A massive Texas w...|A massive Texas w...|https://www.washi...|  MilnerRoute|    1|           0|        1|            0|\n|39950094|       0|            tembo.io|        0|2024-04-06 04:56:39|What’s Happening ...|What’s Happening ...|What's Happening ...|https://tembo.io/...|  samaysharma|    1|           0|        1|            0|\n|39950186|       0|           nautil.us|        0|2024-04-06 05:21:24|The Marvelous Sea...|The Marvelous Sea...|The Marvelous Sea...|https://nautil.us...|      dnetesn|    1|           0|        1|            0|\n|39950205|       0|  worksinprogress.co|        0|2024-04-06 05:25:23|The entrepreneuri...|The entrepreneuri...|The Entrepreneuri...|https://worksinpr...|      sarimkx|    1|           0|        1|            0|\n|39950455|       0|          acoup.blog|        0|2024-04-06 06:25:37|Collections: Phal...|Collections: Phal...|       Antiochus III|https://acoup.blo...|        Tomte|    1|           0|        1|            0|\n|39950521|       0|          castel.dev|        0|2024-04-06 06:47:32|How I draw figure...|How I draw figure...|I draw figures fo...|https://castel.de...|        Tomte|    1|           0|        1|            0|\n|39950540|       0| nathanielkaiser.xyz|        0|2024-04-06 06:53:40|### Did I do this...|                NULL|Show HN: I built ...|https://nathaniel...|       jombib|    1|           0|        1|            0|\n|39950606|       0|chromewebstore.go...|        0|2024-04-06 07:11:04|Skip to main cont...|Upwork Contract E...|Show HN: Export y...|https://chromeweb...|        tamnv|    1|           0|        1|            0|\n|39950637|       0|           gnome.org|        0|2024-04-06 07:18:08|Custom Artifacts ...|Custom Artifacts ...|Gnome Builder Aba...|https://blogs.gno...| todsacerdoti|    1|           0|        1|            0|\n+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+------------+---------+-------------+\nonly showing top 20 rows\n\n+---------+-----+\n|frontpage|count|\n+---------+-----+\n|        1| 1458|\n|        0| 6211|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "df = df.withColumn(\"frontpage\", col(\"frontpage\").cast(\"integer\"))\n",
    "df.groupBy('frontpage').count().show()\n",
    "# Group by 'aid' and find the maximum values for 'comment', 'vote', and 'frontpage'\n",
    "max_values_df = df.groupBy(\"aid\").agg(\n",
    "    max(\"comments\").alias(\"max_comments\"),\n",
    "    max(\"votes\").alias(\"max_votes\"),\n",
    "    max(\"frontpage\").alias(\"max_frontpage\")\n",
    ")\n",
    "\n",
    "# Join the original DataFrame with max_values_df\n",
    "joined_df = df.join(max_values_df, \"aid\")\n",
    "\n",
    "# Filter out rows where 'comment', 'vote', or 'frontpage' are lower than the maximum values\n",
    "filtered_df = joined_df.filter(\n",
    "    (col(\"comments\") == col(\"max_comments\")) &\n",
    "    (col(\"votes\") == col(\"max_votes\")) &\n",
    "    (col(\"frontpage\") == col(\"max_frontpage\"))\n",
    ")\n",
    "\n",
    "df = filtered_df.dropDuplicates([\"aid\"])\n",
    "\n",
    "df.show()\n",
    "# Show the resulting DataFrame\n",
    "df.groupBy('frontpage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6a2875-a905-42b5-90c0-ccb33c8167be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e951d20e9d456caa179a06e1d376a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cf063b3fb14cb09b845f5c1e4bf9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9c91e4c4c543968b88efb2d87ef775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c42a811e004cf68dcb87643da3d9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd86e6ca14d4bbdad0d0410f9490727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cd605e3c214553a3ddb9731f372932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.8875060736195266\nRandom Forest AUC: 0.9229624065596371\nGradient Boosted Trees AUC: 0.9568264803470583\nTest Logistic Regression AUC: 0.9173395194263229\nTest Random Forest AUC: 0.923716143076252\nTest Gradient Boosted Trees AUC: 0.9425614765118919\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, hour, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer, StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Convert posted_at to timestamp\n",
    "df = df.withColumn(\"posted_at\", col(\"posted_at\").cast(\"timestamp\"))\n",
    "df = df.withColumn(\"frontpage\", col(\"frontpage\").cast(\"integer\"))\n",
    "\n",
    "# Extract hour from posted_at\n",
    "hour_udf = udf(lambda x: x.hour if x else None, IntegerType())\n",
    "df = df.withColumn(\"posted_hour\", hour_udf(col(\"posted_at\")))\n",
    "df = df.fillna({\"title\": \"\", \"source_text\": \"\", \"posted_at\": \"\"})\n",
    "\n",
    "# Impute missing values for numerical columns\n",
    "imputer_numeric = Imputer(inputCols=[\"comments\",\"votes\"], outputCols=[\"comments_imputed\",\"votes_imputed\"], strategy=\"mean\")\n",
    "\n",
    "# Standardize numerical columns\n",
    "assembler_numeric = VectorAssembler(inputCols=[\"comments_imputed\", \"votes_imputed\"], outputCol=\"numeric_features\")\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"numeric_features_scaled\")\n",
    "\n",
    "#Text-mining\n",
    "tokenizer_source_text = Tokenizer(inputCol=\"source_text\", outputCol=\"source_text_words\")\n",
    "remover_source_text = StopWordsRemover(inputCol=\"source_text_words\", outputCol=\"source_text_filtered\")\n",
    "hashingTF_source_text = HashingTF(inputCol=\"source_text_filtered\", outputCol=\"source_text_tf\", numFeatures=1024)\n",
    "idf_source_text = IDF(inputCol=\"source_text_tf\", outputCol=\"source_text_tfidf\")\n",
    "\n",
    "tokenizer_title = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "remover_title = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_filtered\")\n",
    "hashingTF_title = HashingTF(inputCol=\"title_filtered\", outputCol=\"title_tf\", numFeatures=1024)\n",
    "idf_title = IDF(inputCol=\"title_tf\", outputCol=\"title_tfidf\")\n",
    "\n",
    "# Assemble all features\n",
    "assembler_all = VectorAssembler(inputCols=[\"posted_hour\", \"numeric_features_scaled\", \"source_text_tfidf\", \"title_tfidf\"],\n",
    "                                outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[imputer_numeric, assembler_numeric, scaler] + \n",
    "                             [tokenizer_source_text, remover_source_text, hashingTF_source_text, idf_source_text,\n",
    "                              tokenizer_title, remover_title, hashingTF_title, idf_title,\n",
    "                              assembler_all])\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_pre_mod, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Split the DataFrame into majority class (frontpage=0) and minority class (frontpage=1)\n",
    "majority_df = train_pre_mod.filter(col(\"frontpage\") == 0)\n",
    "minority_df = train_pre_mod.filter(col(\"frontpage\") == 1)\n",
    "\n",
    "# Sample the majority class DataFrame to match the size of the minority class DataFrame\n",
    "undersampled_majority_df = majority_df.sample(withReplacement=False, fraction=1/4, seed=42)\n",
    "\n",
    "# Concatenate the sampled majority class DataFrame with the minority class DataFrame\n",
    "train = undersampled_majority_df.union(minority_df)\n",
    "\n",
    "# Define models\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "\n",
    "# Create param grids for hyperparameter tuning\n",
    "# Create param grids for hyperparameter tuning\n",
    "paramGridLR = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.1, 0.01])\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5])  # Adding elasticNetParam\n",
    "               .addGrid(lr.maxIter, [10, 20])\n",
    "               .build())\n",
    "\n",
    "paramGridRF = (ParamGridBuilder()\n",
    "               .addGrid(rf.numTrees, [10, 20])  # Adding numTrees\n",
    "               .addGrid(rf.maxDepth, [5, 10])        # Adding maxDepth\n",
    "               .build())\n",
    "\n",
    "paramGridGBT = (ParamGridBuilder()\n",
    "                .addGrid(gbt.maxIter, [10, 20])   # Adding maxIter\n",
    "                .addGrid(gbt.maxDepth, [5, 10])       # Adding maxDepth\n",
    "                .build())\n",
    "\n",
    "# Define evaluators\n",
    "#evaluator = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "# Define evaluators\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='frontpage', predictionCol='prediction', metricName='f1')\n",
    "# Define cross-validators\n",
    "crossvalLR = CrossValidator(estimator=lr,\n",
    "                            estimatorParamMaps=paramGridLR,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=5)\n",
    "\n",
    "crossvalRF = CrossValidator(estimator=rf,\n",
    "                            estimatorParamMaps=paramGridRF,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=5)\n",
    "\n",
    "crossvalGBT = CrossValidator(estimator=gbt,\n",
    "                             estimatorParamMaps=paramGridGBT,\n",
    "                             evaluator=evaluator,\n",
    "                             numFolds=5)\n",
    "\n",
    "# Create pipeline models with cross-validation\n",
    "pipelineLR = Pipeline(stages=[pipeline, crossvalLR])\n",
    "pipelineRF = Pipeline(stages=[pipeline, crossvalRF])\n",
    "pipelineGBT = Pipeline(stages=[pipeline, crossvalGBT])\n",
    "\n",
    "# Train models\n",
    "modelLR = pipelineLR.fit(train)\n",
    "modelRF = pipelineRF.fit(train)\n",
    "modelGBT = pipelineGBT.fit(train)\n",
    "\n",
    "# Evaluate models\n",
    "predictionsLR = modelLR.transform(train)\n",
    "predictionsRF = modelRF.transform(train)\n",
    "predictionsGBT = modelGBT.transform(train)\n",
    "\n",
    "aucLR = evaluator.evaluate(predictionsLR)\n",
    "aucRF = evaluator.evaluate(predictionsRF)\n",
    "aucGBT = evaluator.evaluate(predictionsGBT)\n",
    "\n",
    "print(f\"Logistic Regression AUC: {aucLR}\")\n",
    "print(f\"Random Forest AUC: {aucRF}\")\n",
    "print(f\"Gradient Boosted Trees AUC: {aucGBT}\")\n",
    "\n",
    "test_predictionsLR = modelLR.transform(test)\n",
    "test_predictionsRF = modelRF.transform(test)\n",
    "test_predictionsGBT = modelGBT.transform(test)\n",
    "\n",
    "test_aucLR = evaluator.evaluate(test_predictionsLR)\n",
    "test_aucRF = evaluator.evaluate(test_predictionsRF)\n",
    "test_aucGBT = evaluator.evaluate(test_predictionsGBT)\n",
    "\n",
    "print(f\"Test Logistic Regression AUC: {test_aucLR}\")\n",
    "print(f\"Test Random Forest AUC: {test_aucRF}\")\n",
    "print(f\"Test Gradient Boosted Trees AUC: {test_aucGBT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f88a02-cdc5-421d-bf67-af1301fd4f97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to DBFS\n",
    "model_path_LR = \"/dbfs/models/logistic_regression_model\"\n",
    "model_path_RF = \"/dbfs/models/random_forest_model\"\n",
    "model_path_GBT = \"/dbfs/models/gradient_boosted_model\"\n",
    "modelLR.write().overwrite().save(model_path_LR)\n",
    "modelRF.write().overwrite().save(model_path_RF)\n",
    "modelGBT.write().overwrite().save(model_path_GBT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046a5f09-9164-4813-8a9f-27f58f4ff0dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model from DBFS to workspace directory\n",
    "dbutils.fs.cp(model_path_LR, \"dbfs:/FileStore/models/logistic_regression_model\", recurse=True)\n",
    "dbutils.fs.cp(model_path_RF, \"dbfs:/FileStore/models/random_forest_model\", recurse=True)\n",
    "dbutils.fs.cp(model_path_GBT, \"dbfs:/FileStore/models/gradient_boosted_model\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb78f4a-2017-4781-a03c-1399599673e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model       AUC  Accuracy  F1 Score    Recall\n0     Logistic Regression  0.961797  0.914480  0.917340  0.921646\n1           Random Forest  0.974520  0.919457  0.923716  0.905862\n2  Gradient-Boosted Trees  0.982456  0.939819  0.942561  0.926156\nConfusion Matrix for Logistic Regression:\n[[1635  139]\n [  50  386]]\nConfusion Matrix for Random Forest:\n[[1607  167]\n [  11  425]]\nConfusion Matrix for Gradient-Boosted Trees:\n[[1643  131]\n [   2  434]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol='frontpage', predictionCol='prediction')\n",
    "evaluator=BinaryClassificationEvaluator(labelCol='frontpage', metricName='areaUnderROC')\n",
    "# Calculate metrics\n",
    "def evaluate_model(predictions, model_name):\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: 'accuracy'})\n",
    "    f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: 'f1'})\n",
    "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: 'recallByLabel'})\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "metricsLR = evaluate_model(test_predictionsLR, 'Logistic Regression')\n",
    "metricsRF = evaluate_model(test_predictionsRF, 'Random Forest')\n",
    "metricsGBT = evaluate_model(test_predictionsGBT, 'Gradient-Boosted Trees')\n",
    "\n",
    "# Display comparison\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame([metricsLR, metricsRF, metricsGBT])\n",
    "print(results)\n",
    "\n",
    "# Show confusion matrix for each model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_confusion_matrix(predictions):\n",
    "    y_true = predictions.select(\"frontpage\").rdd.flatMap(lambda x: x).collect()\n",
    "    y_pred = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cmLR = compute_confusion_matrix(test_predictionsLR)\n",
    "cmRF = compute_confusion_matrix(test_predictionsRF)\n",
    "cmGBT = compute_confusion_matrix(test_predictionsGBT)\n",
    "\n",
    "print(f\"Confusion Matrix for Logistic Regression:\\n{cmLR}\")\n",
    "print(f\"Confusion Matrix for Random Forest:\\n{cmRF}\")\n",
    "print(f\"Confusion Matrix for Gradient-Boosted Trees:\\n{cmGBT}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-05-20 14:18:51",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
